{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HA4_iKDHwznp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two import statements are for using PyTorch's neural network toolkit:\n",
        "\n",
        "- **`from torch import nn`**  \n",
        "  This imports the `torch.nn` module and lets you access it as `nn`. The module provides all the *building blocks* for neural networks in PyTorch, such as:\n",
        "  - Layers (e.g., `nn.Linear`, `nn.Conv2d`)\n",
        "  - Containers (e.g., `nn.Sequential`)\n",
        "  - Predefined loss functions (e.g., `nn.CrossEntropyLoss`)\n",
        "  - The base class for models (`nn.Module`)\n",
        "\n",
        "- **`from torch.nn import functional as F`**  \n",
        "  This imports the `functional` API and aliases it as `F`. The `F` namespace provides *stateless* functions for operations like:\n",
        "  - Activation functions (e.g., `F.relu`, `F.softmax`, `F.sigmoid`)\n",
        "  - Other layer operations that don’t need to keep parameters or state.\n",
        "\n"
      ],
      "metadata": {
        "id": "-OjIN4LO8-Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def ___init___(self, n_heads, d_embed, in_proj_bias = True, out_proj_bias = True):\n",
        "    \"\"\"n_heads = no of attention heads, d_head = embedding dimension\n",
        "       n_proj_bias, out_proj_bias - indicates whether i/p o/p peojectiosn must have bias \"\"\"\n",
        "    super().__init__()\n",
        "    #calls constructor of parent class -\n",
        "    '''The parent initialization (e.g., from nn.Module) contains all the essential setup\n",
        "     for parameter tracking, submodule registration, device management, and other features\n",
        "      so your custom layer or model integrates seamlessly with PyTorch’s functionality.'''\n",
        "    self.in_proj = nn.Linear(d_embed, 3*d_embed, bias = in_proj_bias)\n",
        "    #projs input into one (Q, K ,V) for ease can be split later for model use\n",
        "    self.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj_bias)\n",
        "    # we proj i/p into one no we put it back while giving as o/p\n",
        "    self.n_heads = n_heads\n",
        "    self.d_embed = d_embed // n_heads\n",
        "    # Slicing hte dimensions per head\n",
        "    # Computes the dimension of each attention head by dividing the total embedding size by the number of heads.\n",
        "\n",
        "  def forward(self, x, causal_mask = False):\n",
        "    # causal mask - future blocking mask, x is i/p tensor\n",
        "    input_shape = x.shape\n",
        "    batch_size, sequence_length, d_embed = input_shape\n",
        "    #wer gonna store the original input shape in a var so that we can later use it to get back o/p\n",
        "    #unpack into 3 (B S D)for easy reference originally this was combiens into 1\n",
        "    interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "    # reshape into 4 (B S D1 D2)\n",
        "    q, k, v = self.in_proj(x).chunk(3, dim = -1)\n",
        "    #projects x into queries, key and values\n",
        "    # value is the original word embeddings\n",
        "    # we later gonna multiply softmaxed vector to v to get new embeddings for the word\n",
        "    q = q.view(interim_shape).transpose(1, 2)\n",
        "    k = k.view(interim_shape).transpose(1, 2)\n",
        "    v = v.view(interim_shape).transpose(1, 2)\n",
        "    # change shape from bsd to interrim\n",
        "    weight = q @ k.transpose(-1, -2)\n",
        "    # @ in pytorch is matrix multiplication\n",
        "    # calculate attention scores Q * K\n",
        "\n",
        "    if causal_mask:\n",
        "      mask = torch.ones_like(weight, dtype = torch.bool).triu(1)\n",
        "      weight.masked_fill_(mask, -torch.inf)\n",
        "      # apply causal mask ie make future i/p - infinity to not affect curr i/p\n",
        "      # y minus infinity so that added softmax probability comes to 1\n",
        "      # softmax of - inf becoems 0 future tokens gets no weights\n",
        "    weight /= math.sqrt(self.d_head)\n",
        "    # divide bt root of dimensionality stabilizes gradient and softmax\n",
        "    # scales down attentionto prevent gradients to becoem too small or usntable\n",
        "    # if we dont do this probabilities will go near 0 or 1 wont spread\n",
        "    weight = F.softmax(weight, dim = -1)\n",
        "    #softmax it\n",
        "    output = weight @ v\n",
        "    #multiply to get new embedding\n",
        "    output = output.transpose(1, 2)\n",
        "    # transpose axes\n",
        "    output = output.reshape(input_shape)\n",
        "    # get o/p into original i/p shape\n",
        "    output = self.out_proj(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GywpHUDZ8y11"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
        "        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)\n",
        "\n",
        "        input_shape = x.shape\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "        # Divide each embedding of Q into multiple heads such that d_heads * n_heads = Dim_Q\n",
        "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        q = self.q_proj(x)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
        "        k = self.k_proj(y)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
        "        v = self.v_proj(y)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) @ (Batch_Size, H, Dim_Q / H, Seq_Len_KV) -> (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV) @ (Batch_Size, H, Seq_Len_KV, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H)\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        output = output.view(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4J46dQV8PTGS"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}